{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8ebb1a-fe58-4475-b5d4-4ec26bab4047",
   "metadata": {},
   "source": [
    "A notebook to run test of time with synthetic data with `before/after` relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2474cb03-3e02-46b9-89e2-2cc988447fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "\n",
    "from video_llama.common.config import Config\n",
    "from video_llama.common.dist_utils import get_rank\n",
    "from video_llama.common.registry import registry\n",
    "from video_llama.conversation.conversation_video import (\n",
    "    Chat, Conversation, default_conversation,SeparatorStyle,conv_llava_llama_2\n",
    ")\n",
    "import decord\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "from video_llama.datasets.builders import *\n",
    "from video_llama.models import *\n",
    "from video_llama.processors import *\n",
    "from video_llama.runners import *\n",
    "from video_llama.tasks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca929f1-2ec8-4f76-a6a5-8a2f68474a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seeds(seed):\n",
    "    seed = seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbe7cdb-9d6f-4913-b0db-cd1956a3d27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cfg_path': './eval_configs/video_llama_eval_only_vl_edited.yaml',\n",
       " 'model_type': 'llama_v2',\n",
       " 'gpu_id': 0,\n",
       " 'options': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_seeds(0)\n",
    "\n",
    "args = dict(\n",
    "    cfg_path=\"./eval_configs/video_llama_eval_only_vl_edited.yaml\",\n",
    "    model_type=\"llama_v2\",\n",
    "    gpu_id=0,\n",
    "    options=[],\n",
    ")\n",
    "args = AttrDict(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fca96e-0148-4b29-a360-7d6231ebed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(args)\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd8b9d5-f31b-4b36-9240-faa1d4fee9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.equip_audio_branch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2690e46-1e25-41b4-9395-97faa5b0210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:32<00:00, 76.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load first Checkpoint: /work/piyush/pretrained_checkpoints/LargeModels/VideoLLAMA/Video-LLaMA-2-7B-Pretrained/VL_LLaMA_2_7B_Pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "device_str = 'cuda:{}'.format(args.gpu_id)\n",
    "model = model_cls.from_config(model_config).to(device_str)\n",
    "model = model.eval()\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.webvid.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(\n",
    "    vis_processor_cfg.name\n",
    ").from_config(vis_processor_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10fc7652-66a9-4702-b880-0712c1b0b3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7.85B\n"
     ]
    }
   ],
   "source": [
    "n_params = np.sum([p.numel() for p in model.parameters()])\n",
    "n_params = n_params / 1e9\n",
    "print(f\"Number of parameters: {np.round(n_params, 2)}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b54076-aa48-4220-aab3-365c256fd2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise chat\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5896fac3-99d1-4e73-8e1a-c31b642ef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_about_video(chat, video_path, question, num_beams=1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    A wrapper function to ask anything about video at given path.\n",
    "    \"\"\"\n",
    "    chat_state = conv_llava_llama_2.copy()\n",
    "    chat_state.system =  \"You are able to understand the visual content that the user provides.\"\\\n",
    "        \"Follow the instructions carefully and explain your answers in detail.\"\n",
    "    img_list = []\n",
    "    llm_message = chat.upload_video_without_audio(\n",
    "        video_path, chat_state, img_list, video_loader=\"load_video\",\n",
    "    )\n",
    "\n",
    "    chat.ask(question, chat_state)\n",
    "    llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000,\n",
    "    )[0]\n",
    "\n",
    "    return llm_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca3f91-d968-4be3-9b62-a9f0bb1dbdd7",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0be8f0-47ac-4004-8b54-857283a5ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8210c55-0ccc-4097-ab3c-5fe5343a035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/scratch/shared/nfs2/piyush/datasets/ToT-syn-v2.0\"\n",
    "\n",
    "video_dir = os.path.join(data_dir, \"videos\")\n",
    "metad_dir = os.path.join(data_dir, \"metadata\")\n",
    "\n",
    "filenames = sorted([x.split(\".mp4\")[0] for x in os.listdir(video_dir)])\n",
    "\n",
    "video_files = [os.path.join(video_dir, x + \".mp4\") for x in filenames]\n",
    "metad_files = [os.path.join(metad_dir, x + \".pt\") for x in filenames]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": filenames,\n",
    "        \"video_path\": video_files,\n",
    "        \"metad_path\": metad_files,\n",
    "    }\n",
    ")\n",
    "df = df[df.video_path.apply(os.path.exists)]\n",
    "df = df[df.metad_path.apply(os.path.exists)]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "531791a5-e3c4-4b68-ace9-6b31e32ca4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"metad_path\"].apply(lambda x: torch.load(x)[\"caption\"])\n",
    "captions = []\n",
    "distractors = []\n",
    "for i in range(len(df)):\n",
    "    path = df.iloc[i].to_dict()[\"metad_path\"]\n",
    "    meta = torch.load(path)\n",
    "    captions.append(meta[\"caption\"])\n",
    "    distractors.append(meta[\"distractor\"])\n",
    "df[\"caption\"] = captions\n",
    "df[\"distractor\"] = distractors\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2fc38b0-c060-44fd-8acc-75ab9d90dd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create question\n",
    "\n",
    "def create_question_answer(caption, distractor):\n",
    "    coin_toss = np.random.uniform(0, 1)\n",
    "\n",
    "    if coin_toss:\n",
    "        options = [caption, distractor]\n",
    "        correct_answer_string = f\"(a)\"\n",
    "    else:\n",
    "        options = [distractor, caption]\n",
    "        correct_answer_string = f\"(b)\"\n",
    "\n",
    "    question = f\"Which of the following accurately described the video? \"\\\n",
    "        f\"You are given two options. (a) {options[0]} and (b) {options[1]}. \"\\\n",
    "        f\"You only need to output either (a) or (b).\"\n",
    "    correct_answer_verbose = f\"{correct_answer_string} {caption}\"\n",
    "    return question, correct_answer_string, correct_answer_verbose\n",
    "    \n",
    "\n",
    "# df[\"question\"] = df[[\"caption\", \"distractor\"]].apply(lambda x: create_question(*x), axis=1)\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i].to_dict()\n",
    "    caption = row[\"caption\"]\n",
    "    distractor = row[\"distractor\"]\n",
    "    q, a_short, a_long = create_question_answer(caption, distractor)\n",
    "    df.at[i, \"question\"] = q\n",
    "    df.at[i, \"ans_short\"] = a_short\n",
    "    df.at[i, \"ans_long\"] = a_long\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c55f-832d-4064-8c65-7e1ae3487985",
   "metadata": {},
   "source": [
    "**Debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02f29084-83ef-45be-91f4-22d370fafa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/scratch/shared/nfs2/piyush/datasets/ToT-syn-v2.0/videos/text_pair_after_1.mp4',\n",
       " 'Which of the following accurately described the video? You are given two options. (a) A yellow circle appears gradually after a red circle and (b) A red circle appears gradually after a yellow circle. You only need to output either (a) or (b).')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "row = df.iloc[i].to_dict()\n",
    "\n",
    "question = row[\"question\"]\n",
    "video_path = row[\"video_path\"]\n",
    "video_path, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b90ed00-3fa0-47fc-891c-ee3d818fb749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided video frames, the accurate description of the video is (a) A yellow circle appears gradually after a red circle.\\nHere's a detailed explanation of each frame:\\n\\n0.0 seconds: The video starts with a red circle in the center of the screen.\\n\\n0.1 seconds: The red circle starts to fade away, and a small yellow circle appears in the center of the screen, gradually increasing in size.\\n\\n0.2 seconds: The yellow circle is now larger than the red circle, and it continues to grow in size.\\n\\n0.4 seconds: The yellow circle has filled the entire screen, and the red circle has completely faded away.\\n\\n0.5 seconds: The yellow circle remains in the center of the screen, and it starts to shrink in size.\\n\\n0.6 seconds: The yellow circle has shrunk to about half its original size.\\n\\n0.7 seconds: The yellow circle has shrunk even further, and it is now just a small circle in the top-left corner of the screen.\\n\\n0.9 seconds: The video ends with the small yellow circle in the top-left corner of the screen.\\n\\nTherefore, the video shows a yellow circle appearing gradually after a red circle, which matches option (a) in the question.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = ask_about_video(chat, video_path, question)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23eb9388-d343-4a0e-83de-c520a10a3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(llm_output, correct_answer):\n",
    "    return int(correct_answer.lower() in llm_output.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8160717a-87e5-41c9-b775-d7487faebea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_answer(llm_output, row[\"ans_long\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59ef28-9091-42b7-afea-fa92cff3de58",
   "metadata": {},
   "source": [
    "**Run on entire dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1068aec1-ee4b-4198-9eee-e2d688b71292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0733a23c-61d1-4ba3-8777-df6c9271a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on the whole set:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 180/180 [36:05<00:00, 12.03s/it]\n"
     ]
    }
   ],
   "source": [
    "iterator = tqdm(range(len(df)), desc=\"Running on the whole set:\")\n",
    "flags = []\n",
    "for i in iterator:\n",
    "    row = df.iloc[i].to_dict()\n",
    "    question = row[\"question\"]\n",
    "    video_path = row[\"video_path\"]\n",
    "    correct_answer = row[\"ans_long\"]\n",
    "\n",
    "    # Predict\n",
    "    llm_output = ask_about_video(chat, video_path, question)\n",
    "    flag = check_answer(llm_output, correct_answer)\n",
    "    flags.append(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f1f193-5665-4ba3-860f-a0e2f1c0843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test of time:  0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test of time: \", np.mean(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d1da7-17dc-48fb-94ad-aa208d0b84cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
